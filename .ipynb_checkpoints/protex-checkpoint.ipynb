{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProtEx: A PROTein EXtension Tool\n",
    "\n",
    "The ultimate goal of this project is to be able to detect selenoproteins in databases which have been incorrectly truncated at the first instance of a Sec codon, which shares the same code (AUG) as the canonical stop codon. For this task, we are planning to apply the pre-trained ESM transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from esm import ESM\n",
    "import main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and preprocessing\n",
    "\n",
    "All amino acid sequences were obtained from the UniProt database. The data is in FASTA format for compatibility with the CD-Hit sequence clustering tool -- I wrote additional functions to read FASTA files into `pandas.DataFrame`s, and vice versa. The datasets used, along with descriptions, are given below.\n",
    "1. `./data/sec_full.fasta` All peptide sequences tagged in the UniProt database as containing a selenocysteine residue. There are about 20,000 of these. \n",
    "2. `./data/sec_trunc.fasta` This contains the same proteins as the `sec_full.fasta` file, but all sequences have been truncated at the first selenocysteine. \n",
    "3. `./data/short.fasta` This dataset serves as true positive instances of short proteins which are *not* incorrectly truncated. This file was generated by using a Gaussian KDE to approximate a length distribution of truncated selenoproteins (in `sec_trunc.fasta`), sampling from the distribution, and downloading sequences from UniProt which match the length. This dataset is of size __. \n",
    "4. `./data/all.fasta` A combination of `sec_trunc.fasta` and `short.fasta`.\n",
    "\n",
    "### Generating train and test data\n",
    "\n",
    "The The CD-Hit protein clustering tool was used to organize the sequences contained in `all.fasta` into clusters with 80 percent similarity. This was done using the command `cd-hit -i all.fasta -o ./clusters/all -n 5 -c 0.8`. Clustering the sequences, and organizing the training and testing sets such that no cluster spans the two, helps to ensure that the test accuracy captures whether or not the model is generalizing (similar to the approach [here](https://www.biorxiv.org/content/10.1101/626507v4.full)). \n",
    "\n",
    "## Sequence classification\n",
    "\n",
    "This part of the project is *very* incomplete as of now. Eventually, I will try each of these classification techniques, and compare. My priority right now is to do the bag-of-words benchline using a simple `torch.Embedding` layer. \n",
    "\n",
    "### Using ESM\n",
    "\n",
    "This portion of code is mostly complete, although I am thinking about making some stylistic changes (e.g. moving the label and data-loading functions outside of the classifier, just so that's more visible in the code, and probably getting rid of some code repetition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# I think I'll probably rename this to be ESMClassifier or something... \n",
    "esm = ESM()\n",
    "prediction, accuracy = esm.classify(read_from='all.fasta')\n",
    "\n",
    "# Another thing I am thinking about doing is moving the data-loading portion out of the classify function... \n",
    "# maybe have a nice class for loading the data which can then be passed into classify, although that migh be overkill. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using \"bag-of-words\" benchmark\n",
    "`#TODO`\n",
    "\n",
    "### Using LSTM\n",
    "`#TODO`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
