{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProtEx: A PROTein EXtension Tool\n",
    "\n",
    "The ultimate goal of this project is to be able to detect selenoproteins in databases which have been incorrectly truncated at the first instance of a Sec codon, which shares the same code (AUG) as the canonical stop codon. For this task, we are planning to apply the pre-trained ESM transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 10:07:23.342187: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-31 10:07:23.495941: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-31 10:07:24.387004: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Fixing some of the issues I'm having with importing modules. \n",
    "import sys\n",
    "sys.path.append('/home/prichter/Documents/protex/src/')\n",
    "\n",
    "from src.utils import fasta_to_df, clstr_to_df\n",
    "from src.dataset import SequenceDataset\n",
    "from src.esm import ESMClassifier, esm_train\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and preprocessing\n",
    "\n",
    "All amino acid sequences were obtained from the UniProt database. The data is in FASTA format for compatibility with the CD-Hit sequence clustering tool -- I wrote additional functions to read FASTA files into `pandas.DataFrame`s, and vice versa. The datasets used, along with descriptions, are given below.\n",
    "1. `./data/sec.fasta` All peptide sequences tagged in the UniProt database as containing a selenocysteine residue (there are about 20,000). All sequences have been truncated at the first selenocysteine. \n",
    "2. `./data/short.fasta` This dataset serves as true positive instances of short proteins which are *not* incorrectly truncated. This file was generated by using a Gaussian KDE to approximate a length distribution of truncated selenoproteins (in `sec_trunc.fasta`), sampling from the distribution, and downloading sequences from UniProt which match the length. This dataset is of size __. \n",
    "3. `./data/all.fasta` A combination of `sec_trunc.fasta` and `short.fasta`. Generated using `cat sec.fasta short.fasta >> all.fasta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using this function already assumes that the selenoprotein data has been downloaded from UniProt.\n",
    "# This function overwrites the original UniProt file with the truncated sequences. \n",
    "data.truncate_selenoproteins('/home/prichter/Documents/protex/data/sec.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the newly-truncated selenoproteins sequences into a DataFrame.\n",
    "sec_data = data.fasta_to_df('/home/prichter/Documents/protex/data/sec.fasta')\n",
    "# Use KDE to generate a distribution based on the lengths of the truncated selenoproteins. \n",
    "lengths = sec_data['seq'].apply(len).to_numpy()\n",
    "dist = stats.gaussian_kde(lengths)\n",
    "\n",
    "data.download_short_proteins(dist, '/home/prichter/Documents/protex/short.fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Generating train and test data\n",
    "\n",
    "The The CD-Hit protein clustering tool was used to organize the sequences contained in `all.fasta` into clusters with 80 percent similarity. This was done using the command `cd-hit -i all.fasta -o all -n 5 -c 0.8`. Clustering the sequences, and organizing the training and testing sets such that no cluster spans the two, helps to ensure that the test accuracy captures whether or not the model is generalizing (similar to the approach [here](https://www.biorxiv.org/content/10.1101/626507v4.full)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the clustering and sequence information. \n",
    "clstr_data = clstr_to_df('/home/prichter/Documents/protex/data/all.clstr')\n",
    "fasta_data = fasta_to_df('/home/prichter/Documents/protex/data/all.fasta')\n",
    "\n",
    "data = clstr_data.merge(fasta_data, how='inner', on='id') # Need to combine the two DataFrames according to the ID column, using intersection of keys from both frames.\n",
    "data = data.drop_duplicates(subset=['id']) # Make sure there are no duplicates (I was running into some issues with this)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a greedy partitioning algorithm, I split the combined dataset into a train and test set (with a roughly 75%/25% train/test split). This partitioning approach ensures that no cluster group spans both train and test datasets, for the reasons mentioned above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, train_size=0.75, test_size=0.25)\n",
    "\n",
    "# Confirm that the split sizes are correct. \n",
    "train_size = int(len(train_data) / len(data) * 100)\n",
    "test_size = int(len(test_data) / len(data) * 100)\n",
    "\n",
    "print(f'The training dataset makes up {train_size} percent of the total data.')\n",
    "print(f'The testing dataset makes up {test_size} percent of the total data.')\n",
    "\n",
    "# Confirm that there is no cluster overlap. \n",
    "train_clusters = set(train_data['cluster'])\n",
    "test_clusters = set(test_data['cluster'])\n",
    "assert len(train_clusters) + len(test_clusters) == len(train_clusters.union(test_clusters))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating the train and test datasets, I saved the data to CSV files to avoid having to re-run cells more than I need to. These files are also stored in the data directory, and contain the amino acid sequences, unique identifiers, and cluster number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('/home/prichter/Documents/protex/data/train.csv', columns=['seq', 'id', 'cluster'])\n",
    "test_data.to_csv('/home/prichter/Documents/protex/data/test.csv', columns=['seq', 'id', 'cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sequence classification\n",
    "\n",
    "This part of the project is *very* incomplete as of now. Eventually, I will try each of these classification techniques, and compare. My priority right now is to do the bag-of-words benchline using a simple `torch.Embedding` layer. \n",
    "\n",
    "### Using ESM\n",
    "\n",
    "This portion of code is mostly complete, although I am thinking about making some stylistic changes (e.g. moving the label and data-loading functions outside of the classifier, just so that's more visible in the code, and probably getting rid of some code repetition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SequenceDataset(pd.read_csv('/home/prichter/Documents/protex/data/train.csv', nrows=1000))\n",
    "# test_data = SequenceDataset(pd.read_csv('/home/prichter/Documents/protex/data/test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing EsmForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing EsmModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing EsmModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.weight', 'esm.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_v1 = ESMClassifier(use_builtin_classifier=True)\n",
    "model_v2 = ESMClassifier(use_builtin_classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_v1 = esm_train(model_v1, train_data, batch_size=100, n_epochs=5)\n",
    "torch.save(model_v1, 'model_v1.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training classifier...:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "losses_v2 = esm_train(model_v2, train_data, batch_size=100, n_epochs=5)\n",
    "torch.save(model_v2, 'model_v2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builtin_model_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "#dir(transformers.EsmForSequenceClassification.from_pretrained(\"facebook/esm2_t6_8M_UR50D\"))\n",
    "params = transformers.EsmModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "dir(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in params:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using \"bag-of-words\" benchmark\n",
    "`#TODO`\n",
    "\n",
    "### Using LSTM\n",
    "`#TODO`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/esm2_t6_8M_UR50D')\n",
    "encoding = tokenizer([['MPSMSRRQFLKVTGTTLVGSSLALMGFAPGIALAEVRQYKLTRATETRNTCTYCSVACGI', 'FAPGIALAEVRQYKLTRATETRNTCTYCSVACGI']], return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
